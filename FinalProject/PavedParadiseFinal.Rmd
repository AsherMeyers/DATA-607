---
title: 'Paved Paradise: 2014-2016'
author: "Asher Meyers"
date: "May 14, 2016"
output: html_document
---

```{r setup, include=FALSE}

require(stringr)
require(data.table)
```

This is my code for retrieving, updating and formatting the parking data of Santa Monica's Open Data portal.

I ran the scripts below, and then created multiple dashboards in Tableau. When I want to update the data, I run the scripts and then open the dashboards in Tableau and refresh the data source.


###Update Parking Raw Data File
```{r, warning = FALSE}
APILink <- "https://data.smgov.net/resource/tce2-7ir6.csv" #Location of API
AppToken <- "QT8fWTQjKdIwy22DoFZ0bfuEU" #API token
options(scipen=999) #Turn off scientific notation so that API call works.

ParkingRaw <- data.frame() #empty dataframe to hold the raw data
page <- 0 #Page of API results
observations <- 50000 #Number of observations per API call (the maximum)
while (observations == 50000) {
  ParkingRawURL <- paste0(APILink,
                       "?$select=date_time,%20lot_name,%20available_spaces",
                       "&$order=date_time%20ASC,%20lot_name%20ASC",
                       "&$limit=50000",
                       "&$offset=", page*50000) #URL for API call
  ParkingRawPage <- read.csv(url(ParkingRawURL), row.names = NULL, stringsAsFactors = FALSE) #Make the API call and download it to a csv.
  ParkingRaw <- rbind(ParkingRaw, ParkingRawPage) #Bind the current page to the previous pages
  observations <- nrow(ParkingRawPage) #As long as there are more pages of observations, there will be 50,000 observations per page
  page <- page + 1 #on to the next page
  #Get a drink, this might take a while.
}

write.csv(ParkingRaw, file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/ParkingRaw3.csv", row.names = FALSE) #This is for local backup, and not essential to obtaining the results.

ParkingData <- ParkingRaw[ , c(2,3,1)] #Reorder columns to date.time, lot, and spaces available.
names(ParkingData) <- c("Date.Time","Lot","Available") #Rename columns to match original dataset.

ParkingData <- unique(ParkingData) #Remove duplicate rows
ParkingData$Date.Time <- format(strptime(ParkingData$Date.Time,"%Y-%m-%dT%H:%M:%S"), "%m/%d/%Y %H:%M:%S") #Convert date format to match original

BadTimestamp <- names(sort(table(ParkingData$Date.Time),decreasing=TRUE)[1]) #Reveals that one timestamp has more than one entry per parking lot - we'll remove this timestamp. 
ParkingData <- subset(ParkingData, Date.Time != BadTimestamp) #Removes bad timestamp, at Nov 1, 2015, 1 AM. Now, all timestamps have exactly 18 readings - one per garage.

#Names of Lots
ParkingLotsOrig <- sort(unique(ParkingData$Lot)) #Parking Lot names in alphabetical order, original
ParkingLots <- gsub(" ","", ParkingLotsOrig)#Remove spaces from parking names, so they can be used as column names.


ParkingDataM <- data.frame("Date.Time" = unique(ParkingData$Date.Time))#Create a dataframe to house the merged dataset, starting with a vector of 144,849 unique timestamps

#Use a for loop to subset the data by garage, and turn it into a column to add to a dataframe.
for (i in 1:length(ParkingLots)) {
   ParkingDataM <- cbind(ParkingDataM, subset(ParkingData, Lot == ParkingLotsOrig[i])[, 3])
  }
names(ParkingDataM) <- c("Date.Time", ParkingLots) #Rename columns

ParkingDataM$EpochTime <- strptime(ParkingDataM$Date.Time, "%m/%d/%Y %H:%M:%S") #Convert timestamps to Epoch Time format
ParkingDataM$Year <- strptime(ParkingDataM$EpochTime, "%Y-%m-%d")$year+1900 #Add 4 digit year column
ParkingDataM$Month <- as.integer(strptime(ParkingDataM$EpochTime, "%Y-%m-%d")$mon+1) #Obtain the month of a given date, 1-12
ParkingDataM$DayM <- as.integer(strptime(ParkingDataM$EpochTime, "%Y-%m-%d")$mday) #Add day of the month numbering
ParkingDataM$DayNum <- strptime(ParkingDataM$EpochTime, "%Y-%m-%d")$yday+1 #Obtain the day number of the year, eg Feb 1 = 32 
ParkingDataM$Weekday <- strptime(ParkingDataM$EpochTime, "%Y-%m-%d")$wday #Obtain the weekday, 0-6, 0 = Sunday
ParkingDataM$Hour <- strptime(ParkingDataM$EpochTime, "%Y-%m-%d %H:%M")$hour #Obtain the hour of the day
ParkingDataM$Minute <- strptime(ParkingDataM$EpochTime, "%Y-%m-%d %H:%M")$min #Obtain the minute of the day


#Redefine ParkingDataM to put date columns at front and remove original Date.Time column
ParkingDataM <- data.frame(ParkingDataM[ , c(20:27, 2:19)])


#Add Total Column
ParkingDataM$Total <- rowSums(ParkingDataM[ , 9:26])

MaxCapacity <- as.integer(sapply(ParkingDataM[, 9:26], max)) #Operationally define max capacity as maximum available in 2015
VacancyDataAll2M <- round(ParkingDataM[9:26] / MaxCapacity[col(ParkingDataM[, 9:26])], 3) #Divide number of spaces available by max capacity vector
VacancyRatesAll2M <- data.frame(ParkingDataM[ , 1:8], VacancyDataAll2M) #Create a dataset containing the vacancy rate of each garage at each moment
VacancyRatesAll2M$Total <- round(ParkingDataM$Total / sum(MaxCapacity),3)
```

###Update the Weather Raw Data File

```{r, warning = FALSE}

#Download CSVs for each year of data covered by the garages - 2014-2016
WeatherDataRaw14 <- read.csv(url("https://www.wunderground.com/history/airport/KSMO/2014/1/1/CustomHistory.html?dayend=31&monthend=12&yearend=2014&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1"), stringsAsFactors = FALSE)
WeatherDataRaw15 <- read.csv(url("https://www.wunderground.com/history/airport/KSMO/2015/1/1/CustomHistory.html?dayend=31&monthend=12&yearend=2015&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1"), stringsAsFactors = FALSE)
WeatherDataRaw16 <- read.csv(url("https://www.wunderground.com/history/airport/KSMO/2016/1/1/CustomHistory.html?dayend=31&monthend=12&yearend=2016&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1"), stringsAsFactors = FALSE)
names(WeatherDataRaw16)[names(WeatherDataRaw16) == "PDT"] <- "PST" #ensure continuity of column names


WeatherData <- rbind(WeatherDataRaw14, WeatherDataRaw15, WeatherDataRaw16) #Combine the years together
WeatherData$PST <- format(as.POSIXct(strptime(WeatherData$PST, "%Y-%m-%d")), "%m/%d/%Y") #convert dates to standard format
WeatherData <- WeatherData[ , c(1:4, 20, 21, 22)] #Select columns relating to date, temperatures, rainfall, cloudiness and weather events


WeatherData$Year <- as.integer(str_sub(WeatherData$PST, -4, -1)) #add year column
WeatherData$Month <- as.integer(str_extract(WeatherData$PST,"[:digit:]{1,2}")) #add month column
WeatherData$DayM <- as.integer(str_extract(str_extract(WeatherData$PST,"[:digit:]{1,2}[:print:]{1}[:digit:]{4}"), "[:digit:]{1,2}")) #add day of month column
WeatherData$RainfallIn <- round(WeatherData$Precipitationmm/25.4, 1) #convert rainfall to inches
WeatherData <- WeatherData[ , c(8:10, 2:4, 6:7, 11)] #Remove original date column and the rainfall in mm column

VacancyWeatherAll <- merge(VacancyRatesAll2M, WeatherData) #merge data on the columns they have in column, inner join.
VacancyWeatherAll <- subset(VacancyWeatherAll, Minute %% 5 == 0) #Remove erratic timestamps that don't end in 5 minute increments.
VacancyWeatherHourly <- subset(VacancyWeatherAll, Minute == 0 & Hour > 7) #Make an abbreviated dataset for easier downloading.

write.csv(VacancyWeatherAll, file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/VacancyWeather14To16.csv", row.names = FALSE) #For local backup to use in creating Tableau dashboard
write.csv(VacancyWeatherHourly, file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/VacancyWeatherHourly.csv", row.names = FALSE) #For posting on Github.

```
