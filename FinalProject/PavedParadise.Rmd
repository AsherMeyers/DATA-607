---
title: 'Final Project: Santa Monica''s Paved Paradise'
author: "Asher Meyers"
date: "April 17, 2016"
output: html_document
---

https://data.smgov.net/resource/tce2-7ir6.json

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(RJSONIO)
library(RCurl)
library(stringr)
library(plyr)
```

##Paved Paradise: Santa Monica's Parking Lots

##Exploratory Data Analysis

Let's start with the library. We'll bring year 2015's data

https://data.smgov.net/Transportation/Parking-Lot-Counts/ng8m-khuz

```{r}
projectDir <- "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/"
Lib2015 <- read.csv(file = paste0(projectDir,"Library2015.csv"), stringsAsFactors = FALSE) #Read in 2015 data for Library

LibSize <- max(Lib2015$Available) #Maximum number of spaces available = 532
minLibSpaces <- min(Lib2015$Available) #Minimum number of spaces available = 0

LibDates <- str_extract(Lib2015[,1],"[:print:]{4,6}[:digit:]{4}") #Extract date of recording
LibTimes <- str_extract(Lib2015[,1],"[:digit:]{1,2}:[:digit:]{2}")

Weekday <- c("Wed", "Thu","Fri", "Sat", "Sun", "Mon", "Tue")
Month <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

LibDayNum <- strptime(LibDates[], "%m/%d/%Y")$yday+1

LibraryDF <- data.frame("month" = Month[as.numeric(str_extract(Lib2015[,1],"[:digit:]{1,2}"))],
                        "dayNum" = LibDayNum, 
                        "weekday" = Weekday[(LibDayNum[] %% 7)+1], 
                        "hour" = str_sub(LibTimes, 1, nchar(LibTimes)-3), #Hour of recording
                        "minute" = str_sub(LibTimes, -2, -1), #Minute of recording
                        "vacancyRate" = round(Lib2015[,2]/LibSize, digits = 2), #Vector of vacancy rates, in %
                        "SpacesAvailable" = Lib2015$Available)

write.csv(LibraryDF, file = paste0(projectDir,"LibraryDF.CSV"))




LibFull <- subset(Lib2015, Lib2015$Available == 0)

```


Let's retrieve our dataset. For simplicity, I downloaded it from the website, instead of making thousands of API calls. My training set is all the data from the year 2015, resulting in a 200MB file.

I then whittle it down to the relevant columns: the parking structure, the time and date, and the number of spaces available.
```{r}
projectDir <- "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/"
ParkingRaw <- read.csv(file = paste0(projectDir,"ParkingRaw.csv"), stringsAsFactors = FALSE) #Retrieve the raw dataset for 2015 (200 MB file, but mostly redundant columns)
ParkingData <- ParkingRaw[ , c(1, 2, 8)] #Whittle down the dataset to three columns: Date, location, available)
```


###Data Assembly

We'll reformat the data into the most simple, serviceable form - a date and time column, and a column for each structure indicating how many spaces are available at that structure.

We'll write these to separate files, so we won't have to use the original unwieldy dataset. To write these repetitious commands, I used Excel and its concatenate function.

```{r}
#Subset the big original dataset into data by structure
BeachHouse <- subset(ParkingData, ParkingData$Lot == "Beach House Lot")[ ,c(1,3)]
Civic <- subset(ParkingData, ParkingData$Lot == "Civic Center")[ ,c(1,3)]
Library <- subset(ParkingData, ParkingData$Lot == "Library")[ ,c(1,3)]
Lot1N <- subset(ParkingData, ParkingData$Lot == "Lot 1 North")[ ,c(1,3)]
Lot3N <- subset(ParkingData, ParkingData$Lot == "Lot 3 North")[ ,c(1,3)]
Lot4S <- subset(ParkingData, ParkingData$Lot == "Lot 4 South")[ ,c(1,3)]
Lot5S <- subset(ParkingData, ParkingData$Lot == "Lot 5 South")[ ,c(1,3)]
Lot8N <- subset(ParkingData, ParkingData$Lot == "Lot 8 North")[ ,c(1,3)]
Pier <- subset(ParkingData, ParkingData$Lot == "Pier Deck")[ ,c(1,3)]
Struc1 <- subset(ParkingData, ParkingData$Lot == "Structure 1")[ ,c(1,3)]
Struc2 <- subset(ParkingData, ParkingData$Lot == "Structure 2")[ ,c(1,3)]
Struc3 <- subset(ParkingData, ParkingData$Lot == "Structure 3")[ ,c(1,3)]
Struc4 <- subset(ParkingData, ParkingData$Lot == "Structure 4")[ ,c(1,3)]
Struc5 <- subset(ParkingData, ParkingData$Lot == "Structure 5")[ ,c(1,3)]
Struc6 <- subset(ParkingData, ParkingData$Lot == "Structure 6")[ ,c(1,3)]
Struc7 <- subset(ParkingData, ParkingData$Lot == "Structure 7")[ ,c(1,3)]
Struc8 <- subset(ParkingData, ParkingData$Lot == "Structure 8")[ ,c(1,3)]
Struc9 <- subset(ParkingData, ParkingData$Lot == "Structure 9")[ ,c(1,3)]

#Rename the column 'Available' to each structure's name, as each column will be joined together
#later into one grand data frame.

BeachHouse <- rename(BeachHouse, c("Available" = "BeachHouse"))
Civic <- rename(Civic, c("Available" = "Civic"))
Library <- rename(Library, c("Available" = "Library"))
Lot1N <- rename(Lot1N, c("Available" = "Lot1N"))
Lot3N <- rename(Lot3N, c("Available" = "Lot3N"))
Lot4S <- rename(Lot4S, c("Available" = "Lot4S"))
Lot5S <- rename(Lot5S, c("Available" = "Lot5S"))
Lot8N <- rename(Lot8N, c("Available" = "Lot8N"))
Pier <- rename(Pier, c("Available" = "Pier"))
Struc1 <- rename(Struc1, c("Available" = "Struc1"))
Struc2 <- rename(Struc2, c("Available" = "Struc2"))
Struc3 <- rename(Struc3, c("Available" = "Struc3"))
Struc4 <- rename(Struc4, c("Available" = "Struc4"))
Struc5 <- rename(Struc5, c("Available" = "Struc5"))
Struc6 <- rename(Struc6, c("Available" = "Struc6"))
Struc7 <- rename(Struc7, c("Available" = "Struc7"))
Struc8 <- rename(Struc8, c("Available" = "Struc8"))
Struc9 <- rename(Struc9, c("Available" = "Struc9"))

#Combine all dataframes into one, innner joined by their date, and sort by date.
ParkingAll <- Reduce(function(x, y) merge(x, y, by = "Date.Time", all=TRUE), list(BeachHouse, Civic, Library, Lot1N, Lot3N, Lot4S, Lot5S, Lot8N, Pier, Struc1, Struc2, Struc3, Struc4, Struc5, Struc6, Struc7, Struc8, Struc9))

```

###

I've taken the liberty of writing this new dataset to the hard drive, for retrieval later.


```{r}

write.csv(ParkingAll, file = paste0(projectDir, "ParkingAll.csv"))

ParkingAll <- read.csv(file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/ParkingAll.csv", stringsAsFactors = FALSE, sep = ",", header = TRUE)[2:20]
```

###Splice the date format into something more easily searched.

Here, I add date and time fields to make subsetting the data easier. Those fields are:

EpochTime: Standardized date format of "YYYY-MM-DD HH:MM:SS TZ" [TZ defaults to my own timezone, when brought up in R, but not in the CSV itself]
Month: Month, in terms of a number (01-12)
DayNum: Number of the day in the year, so Feb 1 = 32.
Hour: Hour of the day
Minute: Minute, which occur in increments of five; sensors report parking availability every five minutes

```{r}
ParkingAll$EpochTime <- as.POSIXct(strptime(ParkingAll[ ,1], "%m/%d/%Y %I:%M:%S %p"))

ParkingAll$Month <- as.integer(strptime(ParkingAll$EpochTime, "%Y-%m-%d")$mon+1) #Obtain the month of a given date
ParkingAll$DayNum <- strptime(ParkingAll$EpochTime, "%Y-%m-%d")$yday+1 #Obtain the daynumber, eg Feb 1 = 32 
ParkingAll$Weekday <- strptime(ParkingAll$EpochTime, "%Y-%m-%d")$wday+1 #Obtain the weekday, 1-7, 1 = Sunday
ParkingAll$Hour <- strptime(ParkingAll$EpochTime, "%Y-%m-%d %H:%M")$hour
ParkingAll$Minute <- strptime(ParkingAll$EpochTime, "%Y-%m-%d %H:%M")$min

write.csv(ParkingAll, file = paste0(projectDir, "ParkingAll.csv"))
```

Until now, I've used a dataset whose units are the number of spaces available. But we may also want to analyze our data in terms of percent available - in other words, a vacancy rate.

To compute the vacancy rate, we'll divide the number of spaces available at a given moment by the maximum number of available spaces that parking structure has had over the year. Instead of looking up what the structure's nominal or stated maximum capacity is, I chose this approach because there might be spaces that exist on paper but not in reality.

One peril of this approach is that if parking capacities changed over the year, the vacancy rate would be inaccurate. Addressing this would require lots of detailed information - and I simply don't have that information.

This is a simple but arduous calculation, and I shall use a function mentioned by G. Grothendieck, attributed to @DavidArenburg and @akrun.

The format of the datasets will be the columns relating to time and date, followed by information about parking available.

```{r}
MaxCapacity <- as.integer(sapply(ParkingAll[, 2:19], max)) #Operationally define max capacity as maximum available in 2015
VacancyData <- ParkingAll[2:19] / MaxCapacity[col(ParkingAll[, 2:19])] #Divide number of spaces available by max capacity vector
VacancyRates <- data.frame(ParkingAll[ , 20:25], VacancyData) #Create a dataset containing the vacancy rate of each garage at each moment
write.csv(VacancyRates, file = paste0(projectDir, "VacancyRates.csv"))

#To homogenize our datasets, we'll recreate the dataset containing the absolute number of parking spaces as well
VacancyNumbers <- data.frame(ParkingAll[ ,20:25], ParkingAll[2:19])
write.csv(VacancyNumbers, file = paste0(projectDir, "VacancyNumbers.csv"))

#Let's put each dataset in chronological order, starting with January 1, 2015.
VacancyRates <- VacancyRates[order(VacancyRates$DayNum, VacancyRates$Hour, VacancyRates$Minute),]
VacancyNumbers <- VacancyNumbers[order(VacancyNumbers$DayNum, VacancyNumbers$Hour, VacancyNumbers$Minute),]
```


##Data Exploration

Now that we have our data, we can start exploring a number of issues. 


###Are Parking Lots Too Full, Too Empty, or Just Right?
At any given moment, we have a more or less fixed number of parking spaces. If there are more motorists seeking a space than there are spaces available, we'll have to *ration* the supply of parking somehow. We can use one of two tools:

**Prices:** We set a price that's high enough that anyone willing to pay it can get a spot. You can think of this as demand based pricing. Additionally, we can set prices to maximize *revenue and profits* or *usage*. Considering that we're studying public garages whose purpose is to give people access to local beaches and merchants, we'll stick to considering the maximizing usage approach. 

In that case, the optimal price is the lowest price that keeps a few spots in each garage available.

**Queues:** First come, first serve - at the given price, there are more cars seeking spaces than there are spaces in the structure; the structure gets filled up; those who arrive when it's full must detour to a different parking lot.


***


On the surface, it looks like many parking structures, including those in our dataset, use prices to ration supply. But in truth, they use a hodgepodge of both approaches; when prices are sub-optimal and garages get filled up, queuing is being used to allocate spaces.


####Which approach should be used? 

On first glance, making all parking free and thus using a queuing approach maximizes usage of the parking lots, which is one goal of public infrastructure in general. 

The problem with that is that with a queuing approach, you get lots of motorists searching for parking, causing traffic. Studies have found that ["30 percent of the cars in congested downtown traffic were cruising for parking"] (http://www.accessmagazine.org/articles/spring-2011/free-parking-free-markets/)

![](http://www.accessmagazine.org/wp-content/uploads/sites/7/2014/10/Cruising-3.png)

Naturally, it can be hard to forecast demand, and set prices at the optimal levels. But if parking lots routinely get full in a predictable pattern, we can say that the prices are too low.



These datasets, with 618,434 rows, are quite large. There may be times where we will use all the information, but at other times, a limited dataset will do.  

Let's try subsetting our data to a popular time to visit the beach - summer weekends, from 11 AM to 5 PM, looking only hourly at each garage

```{r}

head(subset(VacancyRates, Month > 5 & Month < 9 & Weekday > 5 & Hour > 10 & Hour < 17 ))

```




```{r}



```




```{r}



```

















```{r}
write.csv(BeachHouse, file = paste0(projectDir, "BeachHouse.csv"))
write.csv(Civic, file = paste0(projectDir, "Civic.csv"))
write.csv(Library, file = paste0(projectDir, "Library.csv"))
write.csv(Lot1N, file = paste0(projectDir, "Lot1N.csv"))
write.csv(Lot3N, file = paste0(projectDir, "Lot3N.csv"))
write.csv(Lot4S, file = paste0(projectDir, "Lot4S.csv"))
write.csv(Lot5S, file = paste0(projectDir, "Lot5S.csv"))
write.csv(Lot8N, file = paste0(projectDir, "Lot8N.csv"))
write.csv(Pier, file = paste0(projectDir, "Pier.csv"))
write.csv(Struc1, file = paste0(projectDir, "Struc1.csv"))
write.csv(Struc2, file = paste0(projectDir, "Struc2.csv"))
write.csv(Struc3, file = paste0(projectDir, "Struc3.csv"))
write.csv(Struc4, file = paste0(projectDir, "Struc4.csv"))
write.csv(Struc5, file = paste0(projectDir, "Struc5.csv"))
write.csv(Struc6, file = paste0(projectDir, "Struc6.csv"))
write.csv(Struc7, file = paste0(projectDir, "Struc7.csv"))
write.csv(Struc8, file = paste0(projectDir, "Struc8.csv"))
write.csv(Struc9, file = paste0(projectDir, "Struc9.csv"))
```





```{r}
mean(subset(LibraryDF, weekday %in% c("Sat", "Sun") & hour == 12 & month %in% c("Jun", "Jul", "Aug"), select = c(vacancyRate))$vacancyRate) #Average vacancy rate, for Sat and Sun around noon in summer months

Lots <- sort(unique(ParkingData$Lot))
```


https://data.smgov.net/Transportation/Parking-Lot-Counts/ng8m-khuz

API documentation here: https://dev.socrata.com/consumers/getting-started.html

,"&$ORDER%20BY%20date_time"

Note:

%20 = space character
%27 = quote character i.e. this: '
```{r}
projectDir <- "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/"



CountsURL <- "https://data.smgov.net/resource/tce2-7ir6.csv"
libraryLot <- "Library"
start <- "%272015-01-01T00:00:00%27"
finish <-"%272016-01-01T00:00:00%27"

AppToken <- "QT8fWTQjKdIwy22DoFZ0bfuEU"

CountsURLLibCSV <- paste0(CountsURL,
                       "?lot_name=",libraryLot, 
                       "&$where=date_time%20between%20", start,"%20and%20", finish,
                       "&$select=available_spaces,%20date_time",
                       "&$order=date_time%20ASC",
                       "&$offset=1000")

LibDFCSV <- read.csv(url(CountsURLLib), row.names = NULL, stringsAsFactors = FALSE)
```

```{r cars}
CountsURL <- "https://data.smgov.net/resource/tce2-7ir6.json"
libraryLot <- "Library"
start <- "%272015-01-01T00:00:00%27"
finish <-"%272016-01-01T00:00:00%27"

AppToken <- "QT8fWTQjKdIwy22DoFZ0bfuEU"

CountsURLLib <- paste0(CountsURL,
                       "?lot_name=",libraryLot, 
                       "&$where=date_time%20between%20", start,"%20and%20", finish,
                       "&$select=available_spaces,%20date_time",
                       "&$order=date_time%20ASC",
                       "&$offset=1000")

LibDFJSON <- fromJSON(getURL(CountsURLLib))

```

