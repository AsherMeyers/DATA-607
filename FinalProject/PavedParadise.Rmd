---
title: 'Final Project: Santa Monica''s Paved Paradise'
author: "Asher Meyers"
date: "April 17, 2016"
output: html_document
---

https://data.smgov.net/resource/tce2-7ir6.json

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(RJSONIO)
library(RCurl)
library(stringr)
library(plyr)
library(knitr)
library(RColorBrewer)
library(ggplot2)
```

##Paved Paradise: Santa Monica's Parking Lots

##Exploratory Data Analysis

Let's start with the library. We'll bring year 2015's data

https://data.smgov.net/Transportation/Parking-Lot-Counts/ng8m-khuz

```{r}
projectDir <- "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/"
Lib2015 <- read.csv(file = paste0(projectDir,"Library2015.csv"), stringsAsFactors = FALSE) #Read in 2015 data for Library

LibSize <- max(Lib2015$Available) #Maximum number of spaces available = 532
minLibSpaces <- min(Lib2015$Available) #Minimum number of spaces available = 0

LibDates <- str_extract(Lib2015[,1],"[:print:]{4,6}[:digit:]{4}") #Extract date of recording
LibTimes <- str_extract(Lib2015[,1],"[:digit:]{1,2}:[:digit:]{2}")

Weekday <- c("Wed", "Thu","Fri", "Sat", "Sun", "Mon", "Tue")
Month <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

LibDayNum <- strptime(LibDates[], "%m/%d/%Y")$yday+1

LibraryDF <- data.frame("month" = Month[as.numeric(str_extract(Lib2015[,1],"[:digit:]{1,2}"))],
                        "dayNum" = LibDayNum, 
                        "weekday" = Weekday[(LibDayNum[] %% 7)+1], 
                        "hour" = str_sub(LibTimes, 1, nchar(LibTimes)-3), #Hour of recording
                        "minute" = str_sub(LibTimes, -2, -1), #Minute of recording
                        "vacancyRate" = round(Lib2015[,2]/LibSize, digits = 2), #Vector of vacancy rates, in %
                        "SpacesAvailable" = Lib2015$Available)

write.csv(LibraryDF, file = paste0(projectDir,"LibraryDF.CSV"))




LibFull <- subset(Lib2015, Lib2015$Available == 0)

```


Let's retrieve our dataset. For simplicity, I downloaded it from the website, instead of making thousands of API calls. My training set is all the data from the year 2015, resulting in a 200MB file.

I then whittle it down to the relevant columns: the parking structure, the time and date, and the number of spaces available.
```{r}
projectDir <- "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/"
ParkingRaw <- read.csv(file = paste0(projectDir,"ParkingRaw.csv"), stringsAsFactors = FALSE) #Retrieve the raw dataset for 2015 (200 MB file, but mostly redundant columns)
ParkingData <- ParkingRaw[ , c(1, 2, 8)] #Whittle down the dataset to three columns: Date, location, available)
```


###Data Assembly

We'll reformat the data into the most simple, serviceable form - a date and time column, and a column for each structure indicating how many spaces are available at that structure.

We'll write these to separate files, so we won't have to use the original unwieldy dataset. To write these repetitious commands, I used Excel and its concatenate function.

```{r}
#Subset the big original dataset into data by structure
BeachHouse <- subset(ParkingData, ParkingData$Lot == "Beach House Lot")[ ,c(1,3)]
Civic <- subset(ParkingData, ParkingData$Lot == "Civic Center")[ ,c(1,3)]
Library <- subset(ParkingData, ParkingData$Lot == "Library")[ ,c(1,3)]
Lot1N <- subset(ParkingData, ParkingData$Lot == "Lot 1 North")[ ,c(1,3)]
Lot3N <- subset(ParkingData, ParkingData$Lot == "Lot 3 North")[ ,c(1,3)]
Lot4S <- subset(ParkingData, ParkingData$Lot == "Lot 4 South")[ ,c(1,3)]
Lot5S <- subset(ParkingData, ParkingData$Lot == "Lot 5 South")[ ,c(1,3)]
Lot8N <- subset(ParkingData, ParkingData$Lot == "Lot 8 North")[ ,c(1,3)]
Pier <- subset(ParkingData, ParkingData$Lot == "Pier Deck")[ ,c(1,3)]
Struc1 <- subset(ParkingData, ParkingData$Lot == "Structure 1")[ ,c(1,3)]
Struc2 <- subset(ParkingData, ParkingData$Lot == "Structure 2")[ ,c(1,3)]
Struc3 <- subset(ParkingData, ParkingData$Lot == "Structure 3")[ ,c(1,3)]
Struc4 <- subset(ParkingData, ParkingData$Lot == "Structure 4")[ ,c(1,3)]
Struc5 <- subset(ParkingData, ParkingData$Lot == "Structure 5")[ ,c(1,3)]
Struc6 <- subset(ParkingData, ParkingData$Lot == "Structure 6")[ ,c(1,3)]
Struc7 <- subset(ParkingData, ParkingData$Lot == "Structure 7")[ ,c(1,3)]
Struc8 <- subset(ParkingData, ParkingData$Lot == "Structure 8")[ ,c(1,3)]
Struc9 <- subset(ParkingData, ParkingData$Lot == "Structure 9")[ ,c(1,3)]

#Rename the column 'Available' to each structure's name, as each column will be joined together
#later into one grand data frame.

BeachHouse <- rename(BeachHouse, c("Available" = "BeachHouse"))
Civic <- rename(Civic, c("Available" = "Civic"))
Library <- rename(Library, c("Available" = "Library"))
Lot1N <- rename(Lot1N, c("Available" = "Lot1N"))
Lot3N <- rename(Lot3N, c("Available" = "Lot3N"))
Lot4S <- rename(Lot4S, c("Available" = "Lot4S"))
Lot5S <- rename(Lot5S, c("Available" = "Lot5S"))
Lot8N <- rename(Lot8N, c("Available" = "Lot8N"))
Pier <- rename(Pier, c("Available" = "Pier"))
Struc1 <- rename(Struc1, c("Available" = "Struc1"))
Struc2 <- rename(Struc2, c("Available" = "Struc2"))
Struc3 <- rename(Struc3, c("Available" = "Struc3"))
Struc4 <- rename(Struc4, c("Available" = "Struc4"))
Struc5 <- rename(Struc5, c("Available" = "Struc5"))
Struc6 <- rename(Struc6, c("Available" = "Struc6"))
Struc7 <- rename(Struc7, c("Available" = "Struc7"))
Struc8 <- rename(Struc8, c("Available" = "Struc8"))
Struc9 <- rename(Struc9, c("Available" = "Struc9"))

#Combine all dataframes into one, innner joined by their date, and sort by date.
ParkingAll <- Reduce(function(x, y) merge(x, y, by = "Date.Time", all=TRUE), list(BeachHouse, Civic, Library, Lot1N, Lot3N, Lot4S, Lot5S, Lot8N, Pier, Struc1, Struc2, Struc3, Struc4, Struc5, Struc6, Struc7, Struc8, Struc9))

```

###

I've taken the liberty of writing this new dataset to the hard drive, for retrieval later.


```{r}

write.csv(ParkingAll, file = paste0(projectDir, "ParkingAll.csv"))

ParkingAll <- read.csv(file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/ParkingAll.csv", stringsAsFactors = FALSE, sep = ",", header = TRUE)[2:20]
```

###Splice the date format into something more easily searched.

Here, I add date and time fields to make subsetting the data easier. Those fields are:

EpochTime: Standardized date format of "YYYY-MM-DD HH:MM:SS TZ" [TZ defaults to my own timezone, when brought up in R, but not in the CSV itself]
Month: Month, in terms of a number (01-12)
DayNum: Number of the day in the year, so Feb 1 = 32.
Hour: Hour of the day
Minute: Minute, which occur in increments of five; sensors report parking availability every five minutes

```{r}
ParkingAll$EpochTime <- as.POSIXct(strptime(ParkingAll[ ,1], "%m/%d/%Y %I:%M:%S %p"))

ParkingAll$Month <- as.integer(strptime(ParkingAll$EpochTime, "%Y-%m-%d")$mon+1) #Obtain the month of a given date, 1-12
ParkingAll$DayNum <- strptime(ParkingAll$EpochTime, "%Y-%m-%d")$yday+1 #Obtain the daynumber, eg Feb 1 = 32 
ParkingAll$Weekday <- strptime(ParkingAll$EpochTime, "%Y-%m-%d")$wday+1 #Obtain the weekday, 1-7, 1 = Sunday
ParkingAll$Hour <- strptime(ParkingAll$EpochTime, "%Y-%m-%d %H:%M")$hour
ParkingAll$Minute <- strptime(ParkingAll$EpochTime, "%Y-%m-%d %H:%M")$min

write.csv(ParkingAll, file = paste0(projectDir, "ParkingAll.csv"))
```

Until now, I've used a dataset whose units are the number of spaces available. But we may also want to analyze our data in terms of percent available - in other words, a vacancy rate.

To compute the vacancy rate, we'll divide the number of spaces available at a given moment by the maximum number of available spaces that parking structure has had over the year. Instead of looking up what the structure's nominal or stated maximum capacity is, I chose this approach because there might be spaces that exist on paper but not in reality.

One peril of this approach is that if parking capacities changed over the year, the vacancy rate would be inaccurate. Addressing this would require lots of detailed information - and I simply don't have that information.

This is a simple but arduous calculation, and I shall use a function mentioned by G. Grothendieck, attributed to @DavidArenburg and @akrun.

The format of the datasets will be the columns relating to time and date, followed by information about parking available.

```{r}
MaxCapacity <- as.integer(sapply(ParkingAll[, 2:19], max)) #Operationally define max capacity as maximum available in 2015
VacancyData <- round(ParkingAll[2:19] / MaxCapacity[col(ParkingAll[, 2:19])], 2) #Divide number of spaces available by max capacity vector
VacancyRates <- data.frame(ParkingAll[ , 20:25], VacancyData) #Create a dataset containing the vacancy rate of each garage at each moment


#To homogenize our datasets, we'll recreate the dataset containing the absolute number of parking spaces as well
VacancyNumbers <- data.frame(ParkingAll[ ,20:25], ParkingAll[2:19])


#Let's put each dataset in chronological order, starting with January 1, 2015.
VacancyRates <- VacancyRates[order(VacancyRates$DayNum, VacancyRates$Hour, VacancyRates$Minute),]
VacancyNumbers <- VacancyNumbers[order(VacancyNumbers$DayNum, VacancyNumbers$Hour, VacancyNumbers$Minute),]

write.csv(VacancyRates, file = paste0(projectDir, "VacancyRates.csv"))
write.csv(VacancyNumbers, file = paste0(projectDir, "VacancyNumbers.csv"))
```


##Data Exploration

Now that we have our data, we can start exploring a number of issues. 


###Are Parking Lots Too Full, Too Empty, or Just Right?
At any given moment, we have a more or less fixed number of parking spaces. If there are more motorists seeking a space than there are spaces available, we'll have to *ration* the supply of parking somehow. We can use one of two tools:

**Prices:** We set a price that's high enough that anyone willing to pay it can get a spot. You can think of this as demand based pricing. Additionally, we can set prices to maximize *revenue and profits* or *usage*. Considering that we're studying public garages whose purpose is to give people access to local beaches and merchants, we'll stick to considering the maximizing usage approach. 

In that case, the optimal price is the lowest price that keeps a few spots in each garage available.

**Queues:** First come, first serve - at the given price, there are more cars seeking spaces than there are spaces in the structure; the structure gets filled up; those who arrive when it's full must detour to a different parking lot.


***


On the surface, it looks like many parking structures, including those in our dataset, use prices to ration supply. But in truth, they use a hodgepodge of both approaches; when prices are sub-optimal and garages get filled up, queuing is being used to allocate spaces.


####Which approach should be used? 

On first glance, making all parking free and thus using a queuing approach maximizes usage of the parking lots, which is one goal of public infrastructure in general. 

The problem with that is that with a queuing approach, you get lots of motorists searching for parking, causing traffic. Studies have found that ["30 percent of the cars in congested downtown traffic were cruising for parking"] (http://www.accessmagazine.org/articles/spring-2011/free-parking-free-markets/)

![](http://www.accessmagazine.org/wp-content/uploads/sites/7/2014/10/Cruising-3.png)

Congestion is a pressing issue in cities around the world, and Santa Monica is no exception. The city, along with Venice, is home to the beaches of Los Angeles. Millions of people visit the beaches annually - and many by car that they park nearby. The adjacent streets are nowhere near big enough to allow cars to move at the posted speed limits at popular times, like the summer weekends. 

Therefore, a pricing approach is preferred. With proper pricing, a motorist can be assured that he'll usually find a spot at whatever garage he chooses, except when there are anomalous spikes in demand. Over time, motorists will understand that they generally face a tradeoff, of choosing between a convenient parking space and a cheap one.

Now, we'll explore how often parking garages get filled up. If we find repeated, predictable instances of garages filling up, we can recommend raising prices at those times to reduce the congestion caused by people cruising for parking.

####Examining Vacancy Rates


For now, let's confine our results to a few groupings:

* Black Friday - the 'biggest shopping day of the year', November 27, the 331st day of the year
* Twilight Concert Series: On Thursday nights in mid to late summer, free concerts are held at the Santa Monica Pier
* Commuters - this period will probably consist largely of commuters - people working in downtown Santa Monica
* Summer Weekenders - this segment will consist of people going to Santa Monica for work, for the beach, and for shopping on summer weekends.


While there is an active nightlife in Santa Monica, it is dwarfed in size by the commuter and weekend visitor populations. Anecdotally, it also seems bargoers are more apt to use cabs over personally owned cars and transit.

Let's start with Black Friday, as the analysis is the simplest, and is one day.


###Black Friday

![Black Friday Festivities](http://blogs-images.forbes.com/clareoconnor/files/2014/11/shoppingcover-1940x1293.jpg)

```{r}
VacancyRates <- read.csv(file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/VacancyRates.csv", stringsAsFactors = FALSE)[ , 2:25]
VacancyNumbers <- read.csv(file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/VacancyNumbers.csv", stringsAsFactors = FALSE)[ , 2:25]
HourNames <- c( paste0(seq(1, 11), " AM"), "12 PM", paste0(seq(1, 11), " PM"), "12 AM")
```

```{r}
BlackFriday <- subset(VacancyRates, Minute == 0 & DayNum == 331) #Black Friday vacancy rates, on the hour
View(BlackFriday[ , c(1, 7:24)])
kable(BlackFriday)

BlackFridayM <- data.matrix(BlackFriday[ , (ncol(BlackFriday)-17):ncol(BlackFriday)])
rownames(BlackFridayM) <- c("12 AM", paste0(seq(1, 11), " AM"), "12 PM", paste0(seq(1, 11), " PM"))


BFHeatMap <- heatmap(BlackFridayM, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```

We can narrow down our investigation to the times and places where parking is least available.

Let's focus on structures and hours that have at least one segment with less than 10% vacancy:

```{r}

BlackFridayM2 <- BlackFridayM[apply(BlackFridayM, 1, min) < 0.10, apply(BlackFridayM, 2, min) < 0.10]
BFHeatMap2 <- heatmap(BlackFridayM2, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```

We can see that the parking supply at structure 3, and to a lesser extent, Structure 1, hits zero on Black Friday. In the map below, we can see that these two structures are right next to the main shopping area. 

![Parking Structure 1 & 3](http://i.imgur.com/nwrdmx9.png)

While this is a clear case of underpriced parking, it may not be advisable to raise the price for just this one day - Black Friday tends to draw crowds to chain stores whose goods are available at a number of locations; higher parking prices for one day might scare customers into going elsewhere for this one day. It would also complicate the rate structure for parking.

And for good measure, we'll compare this day to the following Friday, to see if Black Friday is different from a neighboring Friday.

```{r}
NonBlackFriday <- subset(VacancyRates, Minute == 0 & DayNum == 338) #December 4 vacancy rates, on the hour

NonBlackFridayM <- data.matrix(NonBlackFriday[ , (ncol(NonBlackFriday)-17):ncol(NonBlackFriday)])
rownames(NonBlackFridayM) <- c("12 AM", paste0(seq(1, 11), " AM"), "12 PM", paste0(seq(1, 11), " PM"))

NonBlackFridayM2 <- NonBlackFridayM[apply(NonBlackFridayM, 1, min) < 0.10, apply(NonBlackFridayM, 2, min) < 0.10]
NonBFHeatMap2 <- heatmap(NonBlackFridayM2, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```

We see that the same structures were most likely to be full, but only two structures crossed our threshold of a sub-10% vacancy rate, as opposed to 7. Notably, this day was not a holiday.

###Twilight Concert Series

A popular concert series was held from July 9 to September 10 (DayNum = 190, 253), on the Pier at 7 PM. How was parking use impacted?

This time, we'll have to aggregate the information from multiple days' worth of data.

```{r}
ConcertThursday <- subset(VacancyRates, Minute == 0 & Weekday == 4 & DayNum > 189 & DayNum < 254 & Hour > 16)
ConcertThursdayA <- aggregate(ConcertThursday[7:24], list(ConcertThursday$Hour), mean)

ConcertThursdayM <- round(data.matrix(ConcertThursdayA[ , (ncol(ConcertThursdayA)-17):ncol(ConcertThursdayA)]),2)
rownames(ConcertThursdayM) <- c(paste0(seq(5, 11), " PM"))

ConcertThursdayM2 <- ConcertThursdayM[apply(ConcertThursdayM, 1, min) < 0.10, apply(ConcertThursdayM, 2, min) < 0.10]
ConcertThursdayHeatMap <- heatmap(ConcertThursdayM2, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```

We see that structure 3 comes up again as least vacant, but also structures 4 and 8. Structure 8 is located close to the freeway exit as well as the concert - a convenient place to park for concertgoers. 

A brief investigation suggests that the Pier parking deck isn't actually open for public use at this time, which explains why its stated vacancy rate is not less than 10%.


```{r}
VacancyRates <- read.csv(file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/VacancyRates.csv", stringsAsFactors = FALSE)[ , 2:25]
VacancyNumbers <- read.csv(file = "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/VacancyNumbers.csv", stringsAsFactors = FALSE)[ , 2:25]

#At what hour does parking occupancy peak?

Weekdays <- subset(VacancyRates, Minute == 0 & Weekday < 6 )
Weekends <- subset(VacancyRates, Minute == 0 & Weekday > 5 & Month > 5 & Month < 9) #Vacancy rates at each hour for weekends in June, July and August
View(round(aggregate(Weekends[7:24], list(Weekends$Hour), mean),2))

BlackFriday <- subset(VacancyRates, Minute == 0 & DayNum == 331)
BlackFridayA <- round(aggregate(BlackFriday[7:24], list(Weekends$Hour), mean),2)


PeakParking <- subset(VacancyRates, Month > 5 & Month < 9 & Weekday > 5 & Hour > 10 & Hour < 17  & Minute == 0)
PeakParking <- data.frame(PeakParking[ , 1:6], round(PeakParking[ , 7:24], 2))
View(aggregate(PeakParking[, 7:24], list(PeakParking$Weekday, PeakParking$Hour), mean)

)
```


###Commuters

Santa Monica splits its parking prices into two periods, November 1 through March 31, and April 1 through October 31. Some, but not all, price each period differently. We will split our analysis accordingly, into Winter and Summer periods. 

Period 1, Winter: January, February, March, November, December 2015; Weekdays; 7 AM - 6 PM
Period 2, Summer: April - October 2015; Weekdays; 7 AM - 6 PM

```{r}
#CommuterW <- subset(VacancyRates, Minute == 0 & Weekday < 6 & Hour < 19 & Hour > 6 & (Month < 4 | Month > 10))
CommuterW <- subset(VacancyRates, Weekday < 6 & Hour < 19 & Hour > 6 & (Month < 4 | Month > 10))
CommuterWA <- aggregate(CommuterW[7:24], list(CommuterW$Hour), mean)

CommuterWM <- round(data.matrix(CommuterWA[ , (ncol(CommuterWA)-17):ncol(CommuterWA)]),2)
rownames(CommuterWM) <- HourNames[7:18]


CommuterWHeatMap <- heatmap(CommuterWM, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```


###Parking Every 5 Minutes

Instead of aggregating parking use hourly, we can get a resolution 12 times as high, by averaging the vacancy rates for every five minute period. A similar but smoother pattern emerges.

```{r}
CommuterW <- subset(VacancyRates, Weekday < 6 & Hour < 16 & Hour > 10 & Minute %% 5 == 0 & (Month < 4 | Month > 10))
CommuterWA <- aggregate(CommuterW[7:24], list(CommuterW$Minute, CommuterW$Hour), mean)

CommuterWM <- round(data.matrix(CommuterWA[ , (ncol(CommuterWA)-17):ncol(CommuterWA)]),2)
rownames(CommuterWM) <-  c("11 AM", rep("",11), "12 PM", rep("",11), "1 PM", rep("",11), "2 PM", rep("",11), "3 PM", rep("",11))


CommuterWHeatMap <- heatmap(CommuterWM, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```


Across parking structures, it looks as if 1 PM is when peak occupancy occurs during the workday. In this period, only Structure 3 averaged a vacancy rate below 10% in any hour between 7 AM and 6 PM.  However, the average masks day to day variation. We might ask, how often does parking go below 11% vacancy at the peak time of 1 PM? 

```{r}
CommuterPeakW <- subset(CommuterW, Hour == 13 & Minute == 0)
round(colSums(CommuterPeakW[ ,7:24] < 0.11)/nrow(CommuterPeakW), 2)
```

We see that Structure 3 goes below 11% vacancy on 95% of weekdays at 1 PM; Structure 1 and Structure 9 go below 11% vacancy at 1 PM 48% and 61% of the time, respectively.

How do we keep vacancy rates at 10% or at these structures, to prevent people from cruising for parking? 

A bit of background: Parking demand is coming from two different groups, workers and visitors. Workers stay parked for shifts that last several hours, while visitors linger less, on average. It's not until both groups are using a structure concurrently that a structure gets filled up. This time period runs from approximately 11 AM to 4 PM.

The structures with low vacancies also tend to be very conveniently placed for visitors, as they're close to many shops and restaurants. It's believed that visitors generate more tax revenue, relative to the amount of time they spend - so visitors receive cheaper or free parking if they stay 2.5 hours or less.

The [rate structure](http://www.smgov.net/uploadedFiles/Departments/PCD/Transportation/Motorists-Parking/Parking-Rates.pdf) for the low vacancy lots is as follows:

*90 minutes free parking.
*$1.00 next hour
*Each additional 30 minutes, $1.50
*$14.00 Daily max
*Alternately, $120 monthly, for weekdays, 6 AM - 7 PM

In order to create more vacancy at Structures 1, 3 and 9, we have two options - raise prices on these structures, or make neighboring structures cheaper. Both are worth exploring - perhaps, say, raising prices for Structure 3 to $140 monthly, keeping prices for 1 and 9 unchanged, and reducing prices of the other structures to $100 a month. And since the city wants to appeal to visitors, pricing for them would remain unchanged.

An even more elegant solution is simply eliminating the monthly parking pass, [as a Seattle hospital did](http://usa.streetsblog.org/2015/05/08/how-seattle-childrens-hospital-took-the-lead-on-healthy-transportation/), to reward people for not driving alone to work. As the hospital official said, “There are no monthly parking passes,” she said. “You pay by the day. That monthly pass is really a 30-day investment. It sends a signal to somebody to optimize that investment by getting as much parking as possible by driving.” When parking is paid daily, via a debit-account style system, people can save money by choosing not to drive sporadically. And since prices are ultimately set by demand, the daily rates would become lower once the cheap monthly rate disappeared.  


###Summer Commuter Parking, Every 5 Minutes


```{r}
CommuterS <- subset(VacancyRates, Weekday < 6 & Hour < 18 & Hour > 6 & Minute %% 5 == 0 & (Month > 3 & Month < 11)) #April - October Weekdays
CommuterSA <- aggregate(CommuterS[7:24], list(CommuterS$Minute, CommuterS$Hour), mean)

CommuterSM <- round(data.matrix(CommuterSA[ , (ncol(CommuterSA)-17):ncol(CommuterSA)]),2)
rownames(CommuterSM) <-  c("7 AM", rep("",11), "8 AM", rep("",11), "9 AM", rep("",11), "10 AM", rep("",11), "11 AM", rep("",11), "12 PM", rep("",11), 
                           "1 PM", rep("",11), "2 PM", rep("",11), "3 PM", rep("",11), "4 PM", rep("",11), "5 PM", rep("",11))


CommuterSHeatMap <- heatmap(CommuterSM, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")

CommuterPeakS <- subset(CommuterS, Hour == 13 & Minute == 0)
round(colSums(CommuterPeakS[ ,7:24] < 0.11)/nrow(CommuterPeakS), 2)
```


We see a similar pattern with summer commuters - Structure 3 goes below 11% vacancy on 95% of days at 1 PM, with Structures 1 and 9 reaching low vacancy levels at peak times 45% and 55% of summer workdays, respectively. The prescription remains the same - some mix of increasing monthly prices on these structures and lowering monthly prices at other lots.


```{r}
CommuterWM2 <- CommuterWM[apply(CommuterWM, 1, min) < 0.15, apply(CommuterWM, 2, min) < 0.15]
CommuterWHeatMap2 <- heatmap(CommuterWM2, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")
```



###Summer Weekenders

```{r}
CommuterS <- subset(VacancyRates, Weekday < 6 & Hour < 18 & Hour > 6 & Minute %% 5 == 0 & (Month > 3 & Month < 11)) #April - October Weekdays
CommuterSA <- aggregate(CommuterS[7:24], list(CommuterS$Minute, CommuterS$Hour), mean)

CommuterSM <- round(data.matrix(CommuterSA[ , (ncol(CommuterSA)-17):ncol(CommuterSA)]),2)
rownames(CommuterSM) <-  c("7 AM", rep("",11), "8 AM", rep("",11), "9 AM", rep("",11), "10 AM", rep("",11), "11 AM", rep("",11), "12 PM", rep("",11), 
                           "1 PM", rep("",11), "2 PM", rep("",11), "3 PM", rep("",11), "4 PM", rep("",11), "5 PM", rep("",11))


CommuterSHeatMap <- heatmap(CommuterSM, Rowv=NA, Colv=NA, 
                     col = colorRampPalette(c("red", "orange", "yellow", "green"))(256), 
                     scale="none", margins=c(6,4.5), xlab = "Parking Lot", ylab = "Hour")

CommuterPeakS <- subset(CommuterS, Hour == 13 & Minute == 0)
round(colSums(CommuterPeakS[ ,7:24] < 0.11)/nrow(CommuterPeakS), 2)
```






Naturally, it can be hard to forecast demand, and set prices at the optimal levels. But if parking lots routinely get full in a predictable pattern, we can say that the prices are too low, and queuing occurs.



These datasets, with 618,434 rows, are quite large. There may be times where we will use all the information, but at other times, a limited dataset will do.  

Let's try subsetting our data to a popular time to visit the beach - summer weekends, from 11 AM to 5 PM, looking only hourly at each garage

```{r}

head(subset(VacancyRates, Month > 5 & Month < 9 & Weekday > 5 & Hour > 10 & Hour < 17 ))

```




```{r}



```




```{r}



```

















```{r}
write.csv(BeachHouse, file = paste0(projectDir, "BeachHouse.csv"))
write.csv(Civic, file = paste0(projectDir, "Civic.csv"))
write.csv(Library, file = paste0(projectDir, "Library.csv"))
write.csv(Lot1N, file = paste0(projectDir, "Lot1N.csv"))
write.csv(Lot3N, file = paste0(projectDir, "Lot3N.csv"))
write.csv(Lot4S, file = paste0(projectDir, "Lot4S.csv"))
write.csv(Lot5S, file = paste0(projectDir, "Lot5S.csv"))
write.csv(Lot8N, file = paste0(projectDir, "Lot8N.csv"))
write.csv(Pier, file = paste0(projectDir, "Pier.csv"))
write.csv(Struc1, file = paste0(projectDir, "Struc1.csv"))
write.csv(Struc2, file = paste0(projectDir, "Struc2.csv"))
write.csv(Struc3, file = paste0(projectDir, "Struc3.csv"))
write.csv(Struc4, file = paste0(projectDir, "Struc4.csv"))
write.csv(Struc5, file = paste0(projectDir, "Struc5.csv"))
write.csv(Struc6, file = paste0(projectDir, "Struc6.csv"))
write.csv(Struc7, file = paste0(projectDir, "Struc7.csv"))
write.csv(Struc8, file = paste0(projectDir, "Struc8.csv"))
write.csv(Struc9, file = paste0(projectDir, "Struc9.csv"))
```





```{r}
mean(subset(LibraryDF, weekday %in% c("Sat", "Sun") & hour == 12 & month %in% c("Jun", "Jul", "Aug"), select = c(vacancyRate))$vacancyRate) #Average vacancy rate, for Sat and Sun around noon in summer months

Lots <- sort(unique(ParkingData$Lot))
```


https://data.smgov.net/Transportation/Parking-Lot-Counts/ng8m-khuz

API documentation here: https://dev.socrata.com/consumers/getting-started.html

,"&$ORDER%20BY%20date_time"

Note:

%20 = space character
%27 = quote character i.e. this: '
```{r}
projectDir <- "C:/Users/asher/Documents/Classes/CUNY/DATA 607/Final Project/"



CountsURL <- "https://data.smgov.net/resource/tce2-7ir6.csv"
libraryLot <- "Library"
start <- "%272015-01-01T00:00:00%27"
finish <-"%272016-01-01T00:00:00%27"

AppToken <- "QT8fWTQjKdIwy22DoFZ0bfuEU"

CountsURLLibCSV <- paste0(CountsURL,
                       "?lot_name=",libraryLot, 
                       "&$where=date_time%20between%20", start,"%20and%20", finish,
                       "&$select=available_spaces,%20date_time",
                       "&$order=date_time%20ASC",
                       "&$offset=1000")

LibDFCSV <- read.csv(url(CountsURLLib), row.names = NULL, stringsAsFactors = FALSE)
```

```{r cars}
CountsURL <- "https://data.smgov.net/resource/tce2-7ir6.json"
libraryLot <- "Library"
start <- "%272015-01-01T00:00:00%27"
finish <-"%272016-01-01T00:00:00%27"

AppToken <- "QT8fWTQjKdIwy22DoFZ0bfuEU"

CountsURLLib <- paste0(CountsURL,
                       "?lot_name=",libraryLot, 
                       "&$where=date_time%20between%20", start,"%20and%20", finish,
                       "&$select=available_spaces,%20date_time",
                       "&$order=date_time%20ASC",
                       "&$offset=1000")

LibDFJSON <- fromJSON(getURL(CountsURLLib))

```

