---
title: "Project 3 Data Analysis"
author: "Asher Meyers"
date: "March 25, 2016"
output: html_document
---

```{r}
library("knitr")
```

knitr::opts_chunk$set(echo = TRUE)
## Data Analysis

Now, we have our results, and can commence  analyzing our data. We have gathered the frequency of mentions of each skill on three different dates, March 16, 19 and 20.

We load in the frequency data from our online repository; since the first column is merely line numbers, we can excise that.

```{r}
freq.url <- url("https://raw.githubusercontent.com/RobertSellers/SlackProjects/master/data/frequency_results.csv")

freq <- read.csv(freq.url, stringsAsFactors = FALSE, sep = ",")
freq <- freq[,2:4]

View(freq)
kable(head(freq))
kable(tail(freq))
```

Perusing, the data, we see we have multiple dates worth of data. 

Let's combine that data into one table, along with the respective skill titles.

```{r}

freqPart1 <- subset(freq, dates == "2016-03-16")
freqPart2 <- subset(freq, dates == "2016-03-19")[1:123,]
freqPart3 <- subset(freq, dates == "2016-03-19")[124:246,]
freqPart4 <- subset(freq, dates == "2016-03-20")

skillTitle.url <- url("https://raw.githubusercontent.com/RobertSellers/SlackProjects/master/data/skills.csv") # The URL where the file listing the skill titles is located.
skillTitle <- read.csv(skillTitle.url, stringsAsFactors = FALSE)[1:123,] # Reads the skill titles into R.

```


A few of our skill titles are too long, and will make plotting difficult, so we will shorten them.

```{r}
skillTitle[79, 1] <- "NLP"
skillTitle[3, 1] <- "MapReduce"
View(skillTitle)

freqAllDates <- data.frame(skill_id = freqPart1$skill_id, t_freq = freqPart1$t_freq + freqPart2$t_freq + freqPart3$t_freq + freqPart4$t_freq, skill_title = skillTitle$Skill) # Creates a dataframe containing all the skills, with the frequency counts from each data gathering session summed together

View(freqAllDates)
```

##Skills with Zero Mentions

Looking at the data, we see that a number of skills don't get mentioned at all. How many?

```{r}
zeroFreqs <-subset(freqAllDates, freqAllDates$t_freq == 0) # The subset of frequencies that are zero
nrow(freqAllDates) #The total number of skills
nrow(zeroFreqs) #The number of skills with zero mentions
nrow(zeroFreqs)/nrow(freqAllDates) #The proportion of skills with zero mentions
```

About 40% of the skills we searched for were never mentioned. To reduce the chance of leaving out an important skill, we clearly included lots of skills that were not commonly talked about. That so many of our skills garnered no mention is not a cause for concern here. 

However, it's important to note, that where there is a significant cost associated with gathering data, one must be more judicious about selecting what data to gather - you can't just dream up a Christmas wishlist of variables and ask for it all.


##Most Popular Skills

From here, we'll limit our investigation to the skills with positive frequencies, hereafter freqPositive.

```{r}

freqPositive <- subset(freqAllDates, freqAllDates$t_freq > 0)
freqSort <- freqPositive[order(-freqPositive$t_freq), ] #Sort results by frequency, descending
View(freqSort)

```



```{r}

barplot(freqSort$t_freq, main = "Mentions of Data Science Skills", xlab = "Skill", ylab = "# of mentions")

```

We see that even after we cut out the zero values, there is still a lot of variance in how much each skill is mentioned. Since there are still dozens of skills with positive values, we'll narrow our analysis to the top 15.

```{r, echo = FALSE}
op <- par(mar = c(15,4,8,2) + 0.1)

barplot(freqSort$t_freq[1:15], main = "Mentions of Data Science Skills", ylab = "# of mentions", names.arg = freqSort$skill_title[1:15], las = 2)
par(op)

```

-------------------------------------------------------------------------


##Popularity Contest: Mentions of Data Science Skills in the Press

Now, we have several articles, and the number of times various skills are mentioned in each article. We checked each article against the same list of skills, to count the number of mentions.


```{r}
articleURL <- url("https://raw.githubusercontent.com/RobertSellers/SlackProjects/master/data/Build-URL_DataFrame-Output.csv")
articleData <- read.csv(articleURL, stringsAsFactors = FALSE, sep = ",")
View(head(articleData))
```

We'll want to aggregate the number of mentions in each article to a grand total of sums across articles. We'll use the aggregate function.

```{r}

articleAgg <- aggregate(articleData$ds_freq, by=list(Category= articleData$skill_name), FUN=sum)
names(articleAgg) <- c("skill", "frequency")

View(articleAgg)
```

With a simple function, we've consolidated our ~14,000 lines of data into 149. We can winnow this data down further, by removing the skills that did not garner a single mention.

```{r}

articlePositive <- subset(articleAgg, articleAgg$frequency > 0)
nrow(articlePositive)
View(articlePositive)
```

Now we have 115 skills with at least one mention in the articles we studied. 34 skills out of 149, or 23%, were not mentioned once.

Now, we can start looking at the most frequently mentioned skills. We'll first sort the skills according to their number of mentions, in descending order

```{r}

articleSort <- articlePositive[order(-articlePositive$frequency), ] #Sort results by frequency, descending
articleSort$rank <- seq(1:nrow(articleSort))
View(articleSort)

```

Let's look at it in barplot form:

```{r, echo = FALSE}


barplot(articleSort$frequency, main = "Mentions of Data Science Skills", ylab = "# of mentions", names.arg = articleSort$skill, col = rainbow(nrow(articleSort)))

```

While this is useful for getting a sense of the distribution of skills, there are simply too many skills to put in a single graph. But this graph does convey that some skills get many more mentions than others - even after we've removed the skills with zero mentions.

Let's take a look at the top 15 skills

```{r}
op2 <- par(mar = c(6.5,4,8,2) + 0.1)
barplot(articleSort$frequency[1:15], main = "Mentions of Data Science Skills", ylab = "# of mentions", names.arg = articleSort$skill[1:15], las = 2, cex.names = 0.8, col = rainbow(15))
par(op2)
```

In table form:

```{r}
kable(articleSort[1:15,])
```

## Analysis of Results

We see that big data is a clear winner. But big data has come to be a byword, even a synonym for data science; indeed, one of the main differences between data science and conventional statistical techniques is that data science handles 'big' datasets generated by software, whereas statistics tends to focus on smaller or simpler samples, or otherwise refers to the specific mathematical techniques used to analyze the data - and not the earlier processes of gathering and tidying the data. 

Big data has also become a buzzword in its own right, one of dubious distinction critics say. The work it describes spans the fields of applied math and computer science - so it can't be simply designated as a mere sub-field of one or the other.

Regardless, it's safe to conclude that to be a data scientist, one must be comfortable with big data.

Next, at #2, we have statistics. Statistics is a core competency of data science - as critical as reflexes are to a racecar driver. 

We could go on and classify each skill, but looking at each of these skills, patterns emerge; we can put them into one of two categories:

## Competencies

These are the general subjects and activities that transcend any specific piece of software or field, and are important or essential to being a good data scientist. These are things like fluency in both words, numbers, logic and visuals; the ability to focus, work with a team, and communicate; curiousity and attention to detail. These skills are timeless, at least as long as the data scientists are human. Most of these are general, but some are specific, like machine learning and regression. 

It is possible, however, that some of these skills may wax and wane in importance as the tools for doing data science improve. Perhaps you manage to set up a data collection system where little to no data tidying is needed; or some advanced form of artificial intelligence makes creating compelling charts much easier. Certainly, the relative value of these competencies depends on the subject of your work.


## Tools

These are the specific pieces of software data scientists use to do their jobs - and this is where you'll see the most change over time, in what is and isn't fashionable and in demand. This includes things like R, Hadoop, Python, SQL, Java and other interfaces and languages. Change is especially swift in areas where software can be changed easily - where organizations aren't tied up using legacy hardware and software.

Still, some of these tools are staples of the data scientist, and will be for some time. R and Python are two obvious candidates. And even when such tools are replaced, facility with the old ones will help to understand the new ones. If say, some other database software came to the fore, experience with SQL would ease the transition to the new program.


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
